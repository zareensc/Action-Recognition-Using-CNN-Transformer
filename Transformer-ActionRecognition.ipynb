{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Transformer-ActionRecognition","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"iPJ_nYTbt_Od","colab_type":"code","colab":{}},"source":["import os\n","import numpy as np\n","from PIL import Image\n","from torch.utils import data\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torchvision\n","import torchvision.models as models\n","import torchvision.transforms as transforms\n","from tqdm import tqdm\n","import torch.utils.data as data\n","import matplotlib.pyplot as plt\n","from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n","from sklearn.metrics import accuracy_score\n","import pandas as pd\n","import pickle\n","from torch.autograd import Variable\n","from sklearn.model_selection import train_test_split\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.naive_bayes import MultinomialNB\n","from sklearn.metrics import confusion_matrix\n","import seaborn as sns\n","import warnings\n","import matplotlib.cbook\n","warnings.filterwarnings(\"ignore\",category=matplotlib.cbook.mplDeprecation)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"U3ROhac9z4Vv","colab_type":"code","colab":{}},"source":["def labels2cat(label_encoder, list):\n","    return label_encoder.transform(list)\n","\n","def labels2onehot(OneHotEncoder, label_encoder, list):\n","    return OneHotEncoder.transform(label_encoder.transform(list).reshape(-1, 1)).toarray()\n","\n","def onehot2labels(label_encoder, y_onehot):\n","    return label_encoder.inverse_transform(np.where(y_onehot == 1)[1]).tolist()\n","\n","def cat2labels(label_encoder, y_cat): #categories to label\n","    return label_encoder.inverse_transform(y_cat).tolist()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"68sr-43s0A8U","colab_type":"code","colab":{}},"source":["## ---------------------- Dataloaders ---------------------- ##\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"49osb_FD0HYG","colab_type":"code","colab":{}},"source":["# for CTrans\n","class Dataset_CTrans(data.Dataset): #torch.util.data.Dataset = an abstract class representing a dataset\n","    \"Characterizes a dataset for PyTorch\"\n","    def __init__(self, data_path, folders, labels, frames, transform=None):\n","        \"Initialization\"\n","        self.data_path = data_path #note: all these are objects and therefore can have index\n","        self.labels = labels\n","        self.folders = folders\n","        self.transform = transform\n","        self.frames = frames\n","\n","    def __len__(self):\n","        \"Denotes the total number of samples\"\n","        return len(self.folders)\n","\n","    def read_images(self, path, selected_folder, use_transform):\n","        X = []\n","        for i in self.frames:\n","            image = Image.open(os.path.join(path, selected_folder, 'frame{:06d}.jpg'.format(i)))\n","\n","            if use_transform is not None:\n","                image = use_transform(image)\n","\n","            X.append(image) #add to list X\n","        X = torch.stack(X, dim=0) #elememt wise operation/ concatenates sequence of tensors along a new dimension. Resource: https://towardsdatascience.com/understanding-dimensions-in-pytorch-6edf9972d3be\n","\n","        return X\n","\n","    def __getitem__(self, index):\n","        \"Generates one sample of data\"\n","        # Select sample\n","        folder = self.folders[index]\n","\n","        # Load data\n","        X = self.read_images(self.data_path, folder, self.transform)     # (input) spatial images\n","        y = torch.LongTensor([self.labels[index]])                  # (labels) LongTensor are for int64 instead of FloatTensor\n","\n","        # print(X.shape)\n","        return X, y\n","\n","## ---------------------- end of Dataloaders ---------------------- ##"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"HfJwl_brcsAp","colab_type":"code","outputId":"acd729ec-f456-41eb-c20c-17f45a835c3e","executionInfo":{"status":"ok","timestamp":1579505706533,"user_tz":-360,"elapsed":50089,"user":{"displayName":"Farzeen Naz Promi 1620225042","photoUrl":"","userId":"15141591316994662876"}},"colab":{"base_uri":"https://localhost:8080/","height":122}},"source":["# from google.colab import drive\n","# drive.mount('/content/drive')"],"execution_count":5,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"DZRC61uE0NkK","colab_type":"code","colab":{}},"source":["## -------------------- (reload) model prediction ---------------------- ##\n","\n","def CTrans_final_prediction(model, device, loader):\n","    cnn_encoder, transformer_encoder = model\n","    #eval() Sets the module in evaluation mode. This is equivalent with self.train(False).\n","    cnn_encoder.eval() \n","    transformer_encoder.eval()\n","\n","    all_y_pred = []\n","    with torch.no_grad(): #Context-manager that disabled gradient calculation.\n","        for batch_idx, (X, y) in enumerate(tqdm(loader)):\n","            # distribute data to device\n","            X = X.to(device) #assigning a device\n","            output = transformer_encoder(cnn_encoder(X))\n","            y_pred = output.max(1, keepdim=True)[1]  # location of max log-probability as prediction\n","            all_y_pred.extend(y_pred.cpu().data.squeeze().numpy().tolist())\n","\n","    return all_y_pred\n","\n","## -------------------- end of model prediction ---------------------- ##\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"6RIBsrBc0ZFV","colab_type":"code","colab":{}},"source":["## ------------------------ CTrans module ---------------------- ##\n","\n","def conv2D_output_size(img_size, padding, kernel_size, stride):\n","    # compute output shape of conv2D\n","    outshape = (np.floor((img_size[0] + 2 * padding[0] - (kernel_size[0] - 1) - 1) / stride[0] + 1).astype(int), #??\n","                np.floor((img_size[1] + 2 * padding[1] - (kernel_size[1] - 1) - 1) / stride[1] + 1).astype(int))\n","    return outshape\n","\n","# 2D CNN encoder using ResNet-152 pretrained\n","class ResCNNEncoder(nn.Module):\n","    def __init__(self, fc_hidden1=512, fc_hidden2=512, drop_p=0.3, CNN_embed_dim=300):\n","        \"\"\"Load the pretrained ResNet-152 and replace top fc layer.\"\"\"\n","        super(ResCNNEncoder, self).__init__()\n","\n","        self.fc_hidden1, self.fc_hidden2 = fc_hidden1, fc_hidden2\n","        self.drop_p = drop_p\n","\n","        resnet = models.resnet152(pretrained=True)\n","        modules = list(resnet.children())[:-1]      # delete the last fc layer.\n","        self.resnet = nn.Sequential(*modules)\n","        self.fc1 = nn.Linear(resnet.fc.in_features, fc_hidden1)\n","        self.bn1 = nn.BatchNorm1d(fc_hidden1, momentum=0.01)\n","        self.fc2 = nn.Linear(fc_hidden1, fc_hidden2)\n","        self.bn2 = nn.BatchNorm1d(fc_hidden2, momentum=0.01)\n","        self.fc3 = nn.Linear(fc_hidden2, CNN_embed_dim)\n","\n","        \n","    def forward(self, x_3d):\n","        cnn_embed_seq = []\n","        for t in range(x_3d.size(1)):\n","            # ResNet CNN\n","            with torch.no_grad():\n","                x = self.resnet(x_3d[:, t, :, :, :])  # ResNet\n","                x = x.view(x.size(0), -1)             # flatten output of conv\n","\n","            # FC layers\n","            x = self.bn1(self.fc1(x))\n","            x = F.relu(x)\n","            x = self.bn2(self.fc2(x))\n","            x = F.relu(x)\n","            x = F.dropout(x, p=self.drop_p, training=self.training)\n","            x = self.fc3(x)\n","\n","            cnn_embed_seq.append(x)\n","\n","        # swap time and sample dim such that (sample dim, time dim, CNN latent dim)\n","        cnn_embed_seq = torch.stack(cnn_embed_seq, dim=0).transpose_(0, 1)\n","\n","        # cnn_embed_seq: shape=(batch, time_step, input_size)\n","\n","        return cnn_embed_seq\n","\n","\n","class TransformerModel(nn.Module):\n","    def __init__(self, CNN_embed_dim=300, h_Transformer_layers=3, h_Transformer=256, h_FC_dim=128, drop_p=0.3, num_classes=50):\n","        super(TransformerModel, self).__init__()\n","\n","        self.Transformer_input_size = CNN_embed_dim\n","        self.h_Transformer_layers = h_Transformer_layers   \n","        self.h_Transformer = h_Transformer                 \n","        self.h_FC_dim = h_FC_dim\n","        self.drop_p = drop_p\n","        self.num_classes = num_classes\n","\n","\n","        self.transformer = nn.Transformer(\n","            d_model = self.Transformer_input_size,\n","            num_decoder_layers = 0,\n","            dim_feedforward = self.h_Transformer,\n","            dropout = self.drop_p\n","        )\n","\n","        self.fc1 = nn.Linear(self.h_Transformer, self.h_FC_dim)\n","        self.fc2 = nn.Linear(self.h_FC_dim, self.num_classes)\n","\n","    def forward(self, x_Transformer):\n","        \n","        Transformer_out = self.transformer(x_Transformer, x_Transformer) \n","\n","        # FC layers \n","        x = self.fc1(Transformer_out[:, -1, :])  \n","        x = F.relu(x)\n","        x = F.dropout(x, p=self.drop_p, training=self.training)\n","        x = self.fc2(x)\n","\n","        return x\n","\n","## ---------------------- end of CTrans module ---------------------- ##\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"vqEwujGy0pYU","colab_type":"code","colab":{}},"source":["# set path\n","\n","data_path = \"./Dataset/VIPCUPFramesForNanny/\"    # define vip  data path\n","action_name_path = \"./nanny_action_names.pkl\"  # load preprocessed action names\n","save_model_path = \"./ctransformer_Nanny_ckpt\"  # save Pytorch models\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"5MsStjOF0sZ9","colab_type":"code","colab":{}},"source":["# EncoderCNN architecture\n","CNN_fc_hidden1, CNN_fc_hidden2 = 1024, 768\n","CNN_embed_dim = 512   # latent dim extracted by 2D CNN\n","res_size = 224        # ResNet image size\n","dropout_p = 0.6       # dropout probability"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"vwOPj7gJkL85","colab_type":"code","colab":{}},"source":["# Transformer architecture\n","Transformer_hidden_layers = 3\n","Transformer_hidden_nodes = 512\n","Transformer_FC_dim = 256"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Z84H8RF00wDG","colab_type":"code","colab":{}},"source":["# training parameters\n","k =  5          # number of target category\n","epochs = 128\n","batch_size = 100\n","learning_rate = 0.0001\n","log_interval = 10"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"owF9YtfQ0y8u","colab_type":"code","colab":{}},"source":["# Select which frame to begin & end in videos\n","begin_frame, end_frame, skip_frame = 1, 10, 1\n","\n","def train(log_interval, model, device, train_loader, optimizer, epoch):\n","    # set model as training mode\n","    cnn_encoder, transformer_encoder = model\n","    cnn_encoder.train()\n","    transformer_encoder.train()\n","\n","    losses = []\n","    scores = []\n","    N_count = 0   # counting total trained sample in one epoch\n","    for batch_idx, (X, y) in enumerate(train_loader):\n","        # distribute data to device\n","        X, y = X.to(device), y.to(device).view(-1, ) #x is images, y is labels\n","\n","        N_count += X.size(0)\n","\n","        optimizer.zero_grad() #sets gradient of all params to zero\n","        output = transformer_encoder(cnn_encoder(X))   # output has dim = (batch, number of classes)\n","\n","        loss = F.cross_entropy(output, y)\n","        losses.append(loss.item()) #adding all losses\n","\n","        # to compute accuracy\n","        y_pred = torch.max(output, 1)[1]  # y_pred != output\n","        step_score = accuracy_score(y.cpu().data.squeeze().numpy(), y_pred.cpu().data.squeeze().numpy())\n","        scores.append(step_score)         # computed on CPU\n","\n","        loss.backward()\n","        optimizer.step()\n","        print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}, Accu: {:.2f}%'.format(epoch + 1, N_count, len(train_loader.dataset), 100. * (batch_idx + 1) / len(train_loader), loss.item(), 100 * step_score))\n","\n","        # show information\n","        # if (batch_idx + 1) % log_interval == 0:\n","        #     print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}, Accu: {:.2f}%'.format(epoch + 1, N_count, len(train_loader.dataset), 100. * (batch_idx + 1) / len(train_loader), loss.item(), 100 * step_score))\n","            \n","\n","\n","    return losses, scores"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"z8zl3yBF02_J","colab_type":"code","colab":{}},"source":["def validation(model, device, optimizer, test_loader):\n","    # set model as testing mode\n","    cnn_encoder, transformer_encoder = model\n","    cnn_encoder.eval()\n","    transformer_encoder.eval()\n","\n","    test_loss = 0\n","    all_y = []\n","    all_y_pred = []\n","    with torch.no_grad():\n","        for X, y in test_loader:\n","            # distribute data to device\n","            X, y = X.to(device), y.to(device).view(-1, )\n","            int_cnn_encoder_X = cnn_encoder(X).long()\n","            output = transformer_encoder(cnn_encoder(X))\n","\n","            loss = F.cross_entropy(output, y, reduction='sum')\n","            test_loss += loss.item()                 # sum up batch loss\n","            y_pred = output.max(1, keepdim=True)[1]  # (y_pred != output) get the index of the max log-probability\n","\n","            # collect all y and y_pred in all batches\n","            all_y.extend(y)\n","            all_y_pred.extend(y_pred)\n","\n","    test_loss /= len(test_loader.dataset)\n","\n","    # compute accuracy\n","    all_y = torch.stack(all_y, dim=0)\n","    all_y_pred = torch.stack(all_y_pred, dim=0)\n","    test_score = accuracy_score(all_y.cpu().data.squeeze().numpy(), all_y_pred.cpu().data.squeeze().numpy())\n","\n","    # show information\n","    print('\\nTest set ({:d} samples): Average loss: {:.4f}, Accuracy: {:.2f}%\\n'.format(len(all_y), test_loss, 100* test_score))\n","\n","    # save Pytorch models of best record\n","    torch.save(cnn_encoder.state_dict(), os.path.join(save_model_path, 'cnn_encoder_epoch{}.pth'.format(epoch + 1)))  # save spatial_encoder\n","    torch.save(transformer_encoder.state_dict(), os.path.join(save_model_path, 'transformer_encoder_epoch{}.pth'.format(epoch + 1)))  # save motion_encoder\n","    torch.save(optimizer.state_dict(), os.path.join(save_model_path, 'optimizer_epoch{}.pth'.format(epoch + 1)))      # save optimizer\n","    print(\"Epoch {} model saved!\".format(epoch + 1))\n","\n","    return test_loss, test_score"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"oQb_mWDq08lt","colab_type":"code","outputId":"15e01df9-759d-4b08-a574-01e94329ebe4","executionInfo":{"status":"ok","timestamp":1579505714926,"user_tz":-360,"elapsed":846,"user":{"displayName":"Farzeen Naz Promi 1620225042","photoUrl":"","userId":"15141591316994662876"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["# Detect devices\n","use_cuda = torch.cuda.is_available()                   # check if GPU exists\n","device = torch.device(\"cuda\" if use_cuda else \"cpu\") \n","print(device)# use CPU or GPU"],"execution_count":19,"outputs":[{"output_type":"stream","text":["cuda\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"nOU5Vmnp1EMm","colab_type":"code","outputId":"afad4456-5659-4fd0-8eee-6d99c7a4162d","executionInfo":{"status":"ok","timestamp":1579505716568,"user_tz":-360,"elapsed":2048,"user":{"displayName":"Farzeen Naz Promi 1620225042","photoUrl":"","userId":"15141591316994662876"}},"colab":{"base_uri":"https://localhost:8080/","height":85}},"source":["# Data loading parameters\n","params = {'batch_size': batch_size, 'shuffle': True, 'num_workers': 4, 'pin_memory': True} if use_cuda else {}\n","\n","\n","# load actions names\n","with open(action_name_path, 'rb') as f:\n","    action_names = pickle.load(f)\n","\n","# convert labels -> category\n","le = LabelEncoder()\n","le.fit(action_names)\n","\n","# show how many classes there are\n","list(le.classes_)\n","\n","# convert category -> 1-hot\n","action_category = le.transform(action_names).reshape(-1, 1)\n","enc = OneHotEncoder()\n","enc.fit(action_category)\n","\n","\n","actions = []\n","fnames = os.listdir(data_path)\n","\n","all_names = []\n","for f in fnames:\n","    loc1 = f.find('g_')\n","    loc2 = f.find('_a')\n","    actions.append(f[(loc1 + 2): loc2])\n","\n","    all_names.append(f)\n","\n","\n","# list all data files\n","all_X_list = all_names                  # all video file names\n","all_y_list = labels2cat(le, actions)    # all video labels\n"],"execution_count":20,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/_label.py:235: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n","  y = column_or_1d(y, warn=True)\n","/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/_label.py:268: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n","  y = column_or_1d(y, warn=True)\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"u1harJoO1F2G","colab_type":"code","colab":{}},"source":["# train, test split\n","train_list, test_list, train_label, test_label = train_test_split(all_X_list, all_y_list, test_size=0.25, random_state=42)\n","\n","transform = transforms.Compose([transforms.Resize([res_size, res_size]),\n","                                transforms.ToTensor(),\n","                                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n","\n","selected_frames = np.arange(begin_frame, end_frame, skip_frame).tolist()\n","\n","train_set, valid_set = Dataset_CTrans(data_path, train_list, train_label, selected_frames, transform=transform), \\\n","                       Dataset_CTrans(data_path, test_list, test_label, selected_frames, transform=transform)\n","\n","train_loader = data.DataLoader(train_set, **params)\n","valid_loader = data.DataLoader(valid_set, **params)\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"4dmsUraa1MOP","colab_type":"code","outputId":"e95cf3eb-0d15-4078-dbba-b4cf89ce11da","executionInfo":{"status":"ok","timestamp":1579505731372,"user_tz":-360,"elapsed":16197,"user":{"displayName":"Farzeen Naz Promi 1620225042","photoUrl":"","userId":"15141591316994662876"}},"colab":{"base_uri":"https://localhost:8080/","height":68}},"source":["# Create model\n","cnn_encoder = ResCNNEncoder(fc_hidden1=CNN_fc_hidden1, fc_hidden2=CNN_fc_hidden2, drop_p=dropout_p, CNN_embed_dim=CNN_embed_dim).to(device)\n","transformer_encoder = TransformerModel(CNN_embed_dim=CNN_embed_dim, h_Transformer_layers=Transformer_hidden_layers, h_Transformer=Transformer_hidden_nodes, \n","                         h_FC_dim=Transformer_FC_dim, drop_p=dropout_p, num_classes=k).to(device)\n","\n","# Parallelize model to multiple GPUs\n","if torch.cuda.device_count() > 1:\n","    print(\"Using\", torch.cuda.device_count(), \"GPUs!\")\n","    cnn_encoder = nn.DataParallel(cnn_encoder)\n","    transformer_encoder = nn.DataParallel(transformer_encoder)\n","\n","    # Combine all parameters\n","    ctrans_params = list(cnn_encoder.module.fc1.parameters()) + list(cnn_encoder.module.bn1.parameters()) + \\\n","                  list(cnn_encoder.module.fc2.parameters()) + list(cnn_encoder.module.bn2.parameters()) + \\\n","                  list(cnn_encoder.module.fc3.parameters()) + list(transformer_encoder.parameters())\n","\n","elif torch.cuda.device_count() == 1:\n","    print(\"Using\", torch.cuda.device_count(), \"GPU!\")\n","    # Combine all parameters\n","    ctrans_params = list(cnn_encoder.fc1.parameters()) + list(cnn_encoder.bn1.parameters()) + \\\n","                  list(cnn_encoder.fc2.parameters()) + list(cnn_encoder.bn2.parameters()) + \\\n","                  list(cnn_encoder.fc3.parameters()) + list(transformer_encoder.parameters())\n","\n","optimizer = torch.optim.Adam(ctrans_params, lr=learning_rate, weight_decay=0.7)\n"],"execution_count":22,"outputs":[{"output_type":"stream","text":["Downloading: \"https://download.pytorch.org/models/resnet152-b121ed2d.pth\" to /root/.cache/torch/checkpoints/resnet152-b121ed2d.pth\n","100%|██████████| 230M/230M [00:02<00:00, 84.4MB/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Using 1 GPU!\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"lrBFAZ_JB-j7","colab_type":"code","colab":{}},"source":[" # record training process\n","epoch_train_losses = []\n","epoch_train_scores = []\n","epoch_test_losses = []\n","epoch_test_scores = []\n","\n","# start training\n","for epoch in range(epochs):\n","    # train, test model\n","    train_losses, train_scores = train(log_interval, [cnn_encoder, transformer_encoder], device, train_loader, optimizer, epoch)\n","    epoch_test_loss, epoch_test_score = validation([cnn_encoder, transformer_encoder], device, optimizer, valid_loader)\n","\n","    # save results\n","    epoch_train_losses.append(train_losses)\n","    epoch_train_scores.append(train_scores)\n","    epoch_test_losses.append(epoch_test_loss)\n","    epoch_test_scores.append(epoch_test_score)\n","\n","    # save all train test results\n","    A = np.array(epoch_train_losses)\n","    B = np.array(epoch_train_scores)\n","    C = np.array(epoch_test_losses)\n","    D = np.array(epoch_test_scores)\n","    np.save('./tran_epoch_training_losses.npy', A)\n","    np.save('./tran_epoch_training_scores.npy', B)\n","    np.save('./tran_epoch_test_loss.npy', C)\n","    np.save('./tran_epoch_test_score.npy', D)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"niE7CYpzn8Di","colab_type":"code","colab":{}},"source":["# plot\n","fig = plt.figure(figsize=(10, 4))\n","plt.subplot(121)\n","plt.plot(np.arange(1, epochs + 1), A[:, -1])  # train loss (on epoch end)\n","plt.plot(np.arange(1, epochs + 1), C)         #  test loss (on epoch end)\n","plt.title(\"model loss\")\n","plt.xlabel('epochs')\n","plt.ylabel('loss')\n","plt.legend(['train', 'test'], loc=\"upper left\")\n","# 2nd figure\n","plt.subplot(122)\n","plt.plot(np.arange(1, epochs + 1), B[:, -1])  # train accuracy (on epoch end)\n","plt.plot(np.arange(1, epochs + 1), D)         #  test accuracy (on epoch end)\n","plt.title(\"training scores\")\n","plt.xlabel('epochs')\n","plt.ylabel('accuracy')\n","plt.legend(['train', 'test'], loc=\"upper left\")\n","title = \"./fig_UCF101_ResNetCRNN.png\"\n","plt.savefig(title, dpi=600)\n","# plt.close(fig)\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"tAa5Ckba1Z82","colab_type":"code","colab":{}},"source":["#check_prediction\n","\n","\n","# reset data loader\n","all_data_params = {'batch_size': batch_size, 'shuffle': False, 'num_workers': 4, 'pin_memory': True} if use_cuda else {}\n","all_data_loader = data.DataLoader(Dataset_CTrans(data_path, all_X_list, all_y_list, selected_frames, transform=transform), **all_data_params)\n","\n","\n","# reload CTrans model\n","cnn_encoder = ResCNNEncoder(fc_hidden1=CNN_fc_hidden1, fc_hidden2=CNN_fc_hidden2, drop_p=dropout_p, CNN_embed_dim=CNN_embed_dim).to(device)\n","transformer_encoder = TransformerModel(CNN_embed_dim=CNN_embed_dim, h_Transformer_layers=Transformer_hidden_layers, h_Transformer=Transformer_hidden_nodes, h_FC_dim=Transformer_FC_dim, drop_p=dropout_p, num_classes=k).to(device)\n","\n","cnn_encoder.load_state_dict(torch.load(os.path.join(save_model_path, 'cnn_encoder_epoch46.pth')))\n","transformer_encoder.load_state_dict(torch.load(os.path.join(save_model_path, 'transformer_encoder_epoch46.pth')))\n","print('model reloaded!')\n","\n","\n","# make all video predictions by reloaded model\n","print('Predicting all {} videos:'.format(len(all_data_loader.dataset)))\n","all_y_pred = CTrans_final_prediction([cnn_encoder, transformer_encoder], device, all_data_loader)\n","\n","\n","# write in pandas dataframe\n","df = pd.DataFrame(data={'filename': fnames, 'y': cat2labels(le, all_y_list), 'y_pred': cat2labels(le, all_y_pred)})\n","df.to_pickle(\"./vip_video_prediction.pkl\")  # save pandas dataframe\n","# pd.read_pickle(\"./all_videos_prediction.pkl\")\n","print('video prediction finished!')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"QYSA49ggpCic","colab_type":"code","colab":{}},"source":["pd.set_option('display.max_rows', 500)\n","with open(\"./vip_video_prediction.pkl\", 'rb') as f:\n","    prediction = pickle.load(f)   # load actions names\n","print(prediction)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"_mWe_8-PpEeu","colab_type":"code","colab":{}},"source":["confusion_matrix = pd.crosstab(df['y'], df['y_pred'], rownames=['Actual'], colnames=['Predicted'])\n","print (confusion_matrix)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Jv9Gtht0vC80","colab_type":"code","colab":{}},"source":["# Build confusion metrics\n","cm = confusion_matrix(y_true=df['y'], y_pred=df['y_pred'])\n","plt.subplots(figsize=(10,8))\n","# Plot confusion matrix in a beautiful manner\n","ax= plt.subplot()\n","sns.heatmap(cm, annot=True, ax = ax, fmt = 'g'); #annot=True to annotate cells\n","# labels, title and ticks\n","ax.set_xlabel('Predicted', fontsize=12)\n","ax.xaxis.set_label_position('top') \n","ax.xaxis.set_ticklabels(['human-interaction', 'microwave','mobile','paper','read'], fontsize = 9)\n","ax.xaxis.tick_top()\n","ax.set_ylabel('True', fontsize=12)\n","ax.yaxis.set_ticklabels(['human-interaction', 'microwave','mobile','paper','read'], fontsize = 9)\n","\n","plt.show()"],"execution_count":0,"outputs":[]}]}